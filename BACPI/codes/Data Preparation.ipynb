{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAenjwlDFxGg"
   },
   "source": [
    "# Part 1: Prepare the development environment -- only need to run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required packages\n",
    "\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import tarfile\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BVNM203br3c8"
   },
   "outputs": [],
   "source": [
    "# functions for processing the data (i.e. extract interaction and affinity data from zipped files)\n",
    "# the output is train/test csv files\n",
    "\n",
    "def extract_data(measure, file):\n",
    "    data = pd.read_csv(file, sep='\\t', dtype=str)\n",
    "    #print(data.columns)\n",
    "    data = data[['Ligand SMILES', 'BindingDB Target Chain  Sequence', 'pKi_[M]', 'pIC50_[M]', 'pKd_[M]', 'pEC50_[M]']]\n",
    "    data.columns = ['SMILES', 'Sequence', 'Ki', 'IC50', 'Kd', 'EC50']\n",
    "    data = data[['SMILES', 'Sequence', measure]]\n",
    "    if 'train' in file:\n",
    "        data.to_csv('../data/csv/train_' + measure + '.csv', index=None, header=None)\n",
    "    else:\n",
    "        data.to_csv('../data/csv/test_' + measure + '.csv', index=None, header=None)\n",
    "\n",
    "def affinity_data_prepare(dataset):\n",
    "    data_dir = '../data/affinity/' + dataset\n",
    "    if not os.path.isdir(data_dir):\n",
    "        tar = tarfile.open(data_dir + '.tar.xz')\n",
    "        os.mkdir(data_dir)\n",
    "        for name in tar.getnames():\n",
    "            tar.extract(name, '../data/affinity/')\n",
    "        tar.close()\n",
    "\n",
    "    extract_data(dataset, data_dir + '/train')\n",
    "    extract_data(dataset, data_dir + '/test')\n",
    "\n",
    "def interaction_data_prepare(dataset):\n",
    "    if dataset == 'human':\n",
    "        train = ('../data/interaction/' + dataset + '/train.txt')\n",
    "        data = pd.read_csv(train, sep=',', dtype=str)\n",
    "        data.columns = ['SMILES', 'Sequence', 'Target']\n",
    "        data.to_csv('../data/csv/train_human.csv', index=None, header=None)\n",
    "        \n",
    "        test = ('../data/interaction/' + dataset + '/test.txt')\n",
    "        data = pd.read_csv(test, sep=',', dtype=str)\n",
    "        data.columns = ['SMILES', 'Sequence', 'Target']\n",
    "        data.to_csv('../data/csv/test_human.csv', index=None, header=None)\n",
    "    else:\n",
    "        train = ('../data/interaction/' + dataset + '/train.txt')\n",
    "        data = pd.read_csv(train, sep=',', dtype=str)\n",
    "        data.columns = ['SMILES', 'Sequence', 'Target']\n",
    "        data.to_csv('../data/csv/train_celegans.csv', index=None, header=None)\n",
    "        \n",
    "        test = ('../data/interaction/' + dataset + '/test.txt')\n",
    "        data = pd.read_csv(test, sep=',', dtype=str)\n",
    "        data.columns = ['SMILES', 'Sequence', 'Target']\n",
    "        data.to_csv('../data/csv/test_celegans.csv', index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for dataset in ['IC50', 'EC50', 'Ki', 'Kd']:\n",
    "    affinity_data_prepare(dataset)\n",
    "    \n",
    "for dataset in ['human', 'celegans']:\n",
    "    interaction_data_prepare(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You should have 12 csv files.\n",
    "\n",
    "<table>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>test_celegans.csv</td>\n",
    "    <td>test_IC50.csv</td>\n",
    "    <td>train_celegans.csv</td>\n",
    "    <td>train_IC50.csv</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>test_EC50.csv</td>\n",
    "    <td>test_Kd.csv</td>\n",
    "    <td>train_EC50.csv</td>\n",
    "    <td>train_Kd.csv</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>test_human.csv</td>\n",
    "    <td>test_Ki.csv</td>\n",
    "    <td>train_human.csv</td>\n",
    "    <td>train_Ki.csv</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiGDHlIYRZsW"
   },
   "source": [
    "# Part 2: Prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rabin badree\\OneDrive\\Documents\\badreeRoziena\\machineLearning\\project\\github\\data\\csv\n"
     ]
    }
   ],
   "source": [
    "# change the directory to that of the CSV files.\n",
    "\n",
    "current = os.getcwd()\n",
    "os.chdir('../data/csv/')\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HO1AjdcUgM2b",
    "outputId": "4b7f7e73-10b0-44b5-fd98-e5d33a595bc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'O', 'N', 'C', 'O', 'N', 'C', 'C', 'C', 'C', 'C', 'C', 'O', 'Cl', 'N', 'O', 'O', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H']\n",
      "['AROMATIC', 'AROMATIC', 'AROMATIC', 'AROMATIC', 'AROMATIC', 'SINGLE', 'DOUBLE', 'SINGLE', 'DOUBLE', 'SINGLE', 'SINGLE', 'DOUBLE', 'SINGLE', 'SINGLE', 'AROMATIC', 'AROMATIC', 'AROMATIC', 'AROMATIC', 'AROMATIC', 'SINGLE', 'SINGLE', 'SINGLE', 'DOUBLE', 'SINGLE', 'AROMATIC', 'SINGLE', 'AROMATIC', 'SINGLE', 'SINGLE', 'SINGLE', 'SINGLE', 'SINGLE', 'SINGLE', 'SINGLE', 'SINGLE', 'SINGLE', 'SINGLE']\n",
      "[0, 1, 2, 3, 4, 5, 14, 15, 16, 17, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('test_EC50.csv', header=None)\n",
    "primary = data.iloc[-1, :]  # three columns : smiles, sequence, interaction \n",
    "radius, ngram = 2, 3\n",
    "mol = Chem.AddHs(Chem.MolFromSmiles(primary[0]))\n",
    "adjacency = Chem.GetAdjacencyMatrix(mol)\n",
    "\n",
    "print([a.GetSymbol() for a in mol.GetAtoms()])\n",
    "print([str(b.GetBondType()) for b in mol.GetBonds()])\n",
    "print([a.GetIdx() for a in mol.GetAromaticAtoms()])\n",
    "\n",
    "#[int(i) for i in AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=64, useChirality=True).ToBitString()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wr0rPhCVGyx3"
   },
   "source": [
    "### Step 1: Generate the data for the compounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_Ibuw4p2Op9u"
   },
   "outputs": [],
   "source": [
    "def create_atoms(mol, atom_dict):\n",
    "# print(primary[0])    \n",
    "# result: C1=CC=C(C(=C1)C2=C(C(=O)NC2=O)NC3=CC(=C(C=C3)O)Cl)[N+](=O)[O-]\n",
    "\n",
    "  atoms = [a.GetSymbol() for a in mol.GetAtoms()]\n",
    "    \n",
    "# print(atoms, len(atoms)) \n",
    "# result:['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'O', 'N', 'C', 'O', 'N', 'C', 'C', 'C', 'C', 'C', 'C', 'O', 'Cl', 'N', 'O', 'O', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H'] 35\n",
    "# print([a.GetIdx() for a in mol.GetAromaticAtoms()])\n",
    "# result:[0, 1, 2, 3, 4, 5, 14, 15, 16, 17, 18, 19]\n",
    "\n",
    "  for a in mol.GetAromaticAtoms():\n",
    "    i = a.GetIdx()\n",
    "    atoms[i] = (atoms[i], 'aromatic')\n",
    "    \n",
    "# print(atoms)\n",
    "# result:[('C', 'aromatic'), ('C', 'aromatic'), ('C', 'aromatic'), ('C', 'aromatic'), ('C', 'aromatic'), ('C', 'aromatic'), 'C', 'C', 'C', 'O', 'N', 'C', 'O', 'N', ('C', 'aromatic'), ('C', 'aromatic'), ('C', 'aromatic'), ('C', 'aromatic'), ('C', 'aromatic'), ('C', 'aromatic'), 'O', 'Cl', 'N', 'O', 'O', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H']\n",
    "\n",
    "  atoms = [atom_dict[a] for a in atoms]\n",
    "    \n",
    "# print(atoms)\n",
    "# result:[0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 3, 1, 2, 3, 0, 0, 0, 0, 0, 0, 2, 4, 3, 2, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
    "\n",
    "  return atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xzBT17V6IayG"
   },
   "outputs": [],
   "source": [
    "def create_ijbonddict(mol, bond_dict):\n",
    "    \n",
    "# print(primary[0])    \n",
    "# result: C1=CC=C(C(=C1)C2=C(C(=O)NC2=O)NC3=CC(=C(C=C3)O)Cl)[N+](=O)[O-]\n",
    "\n",
    "    i_jbond_dict = defaultdict(lambda: [])\n",
    "    \n",
    "#print([len(mol.GetBonds())])\n",
    "# result: 37\n",
    "\n",
    "    for b in mol.GetBonds():\n",
    "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        bond = bond_dict[str(b.GetBondType())]\n",
    "        i_jbond_dict[i].append((j, bond))\n",
    "        i_jbond_dict[j].append((i, bond))\n",
    "        \n",
    "#print(i_jbond_dict)\n",
    "# {0: [(1, 0), (5, 0), (25, 1)],\n",
    "#  1: [(0, 0), (2, 0), (26, 1)],\n",
    "#  2: [(1, 0), (3, 0), (27, 1)],\n",
    "#  3: [(2, 0), (4, 0), (22, 1)],\n",
    "#  4: [(3, 0), (5, 0), (6, 1)],\n",
    "#  5: [(4, 0), (0, 0), (28, 1)],\n",
    "#  6: [(4, 1), (7, 2), (11, 1)],\n",
    "#  7: [(6, 2), (8, 1), (13, 1)],\n",
    "#  8: [(7, 1), (9, 2), (10, 1)],\n",
    "#  9: [(8, 2)],\n",
    "#  10: [(8, 1), (11, 1), (29, 1)],\n",
    "#  11: [(10, 1), (12, 2), (6, 1)],\n",
    "#  12: [(11, 2)],\n",
    "#  13: [(7, 1), (14, 1), (30, 1)],\n",
    "#  14: [(13, 1), (15, 0), (19, 0)],\n",
    "#  15: [(14, 0), (16, 0), (31, 1)],\n",
    "#  16: [(15, 0), (17, 0), (21, 1)],\n",
    "#  17: [(16, 0), (18, 0), (20, 1)],\n",
    "#  18: [(17, 0), (19, 0), (32, 1)],\n",
    "#  19: [(18, 0), (14, 0), (33, 1)],\n",
    "#  20: [(17, 1), (34, 1)],\n",
    "#  21: [(16, 1)],\n",
    "#  22: [(3, 1), (23, 2), (24, 1)],\n",
    "#  23: [(22, 2)],\n",
    "#  24: [(22, 1)],\n",
    "#  25: [(0, 1)],\n",
    "#  26: [(1, 1)],\n",
    "#  27: [(2, 1)],\n",
    "#  28: [(5, 1)],\n",
    "#  29: [(10, 1)],\n",
    "#  30: [(13, 1)],\n",
    "#  31: [(15, 1)],\n",
    "#  32: [(18, 1)],\n",
    "#  33: [(19, 1)],\n",
    "#  34: [(20, 1)]})\n",
    "\n",
    "# find the atoms that do not have bonds with other atoms \n",
    "\n",
    "    isolate_atoms = set(range(mol.GetNumAtoms())) - set(i_jbond_dict.keys())\n",
    "    bond = bond_dict['nan']\n",
    "    for a in isolate_atoms:\n",
    "        i_jbond_dict[a].append((a, bond))\n",
    "\n",
    "    return i_jbond_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cV3hHD2PSCGK"
   },
   "outputs": [],
   "source": [
    "def atom_features(atoms, i_jbond_dict, radius, edge_dict, fingerprint_dict):    \n",
    "    if (len(atoms) == 1) or (radius == 0):\n",
    "        fingerprints = [fingerprint_dict[a] for a in atoms]\n",
    "    \n",
    "    else:\n",
    "        nodes = atoms\n",
    "        i_jedge_dict = i_jbond_dict\n",
    "        for _ in range(radius):\n",
    "            fingerprints = []\n",
    "# the first radius:\n",
    "# i_jedge_dict has 35 items. The following loop converts the atom position number(from 0 to 34), into the atoms_type_dict number(totally 5 types).\n",
    "# shrink the previous 35 atoms with edges, into 18 types of (atoms,edges)\n",
    "# fingerprint = (atom_type_dict, tuple(sorted(neighbors)))    \n",
    "# neighbor = (atom_type_dict, bond_type_dict)\n",
    "            for i, j_edge in i_jedge_dict.items():\n",
    "                neighbors = [(nodes[j], edge) for j, edge in j_edge]\n",
    "                fingerprint = (nodes[i], tuple(sorted(neighbors)))\n",
    "                fingerprints.append(fingerprint_dict[fingerprint])  # after converting into atom_type, many fingerprint are with the same value\n",
    "# print([str(b.GetBondType()) for b in mol.GetBonds()])\n",
    "# ['AROMATIC', 'AROMATIC', 'AROMATIC', 'AROMATIC', 'AROMATIC', 'SINGLE', 'DOUBLE', 'SINGLE', 'DOUBLE', 'SINGLE', 'SINGLE', 'DOUBLE', 'SINGLE', 'SINGLE', 'AROMATIC', 'AROMATIC', 'AROMATIC', 'AROMATIC', 'AROMATIC', 'SINGLE', 'SINGLE', 'SINGLE', 'DOUBLE', 'SINGLE', 'AROMATIC', 'SINGLE', 'AROMATIC', 'SINGLE', 'SINGLE', 'SINGLE', 'SINGLE', 'SINGLE', 'SINGLE', 'SINGLE', 'SINGLE', 'SINGLE', 'SINGLE']\n",
    "#print(fingerprints, len(fingerprints))\n",
    "# the first radius of fingerprints: \n",
    "#[0, 0, 0, 1, 2, 0, 3, 4, 5, 6, 7, 5, 6, 8, 1, 0, 9, 10, 0, 0, 11, 12, 13, 14, 15, 16, 16, 16, 16, 17, 17, 16, 16, 16, 18] ---len:35\n",
    "# the second radius of fingerprints: \n",
    "#[0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 11, 12, 13, 14, 15, 16, 1, 17, 18, 19, 20, 21, 22, 22, 22, 22, 23, 24, 22, 22, 22, 25]--with atom type and edge type\n",
    "\n",
    "# update the i_jedge_dict by disgarding the duplicated edges for both directions and only keeping one undirected edge     \n",
    "# upgrade the edge type, from the original edge type, to combine both nodes' type and edge type      \n",
    "            nodes = fingerprints\n",
    "            _i_jedge_dict = defaultdict(lambda: [])\n",
    "            for i, j_edge in i_jedge_dict.items():\n",
    "                for j, edge in j_edge:\n",
    "                    both_side = tuple(sorted((nodes[i], nodes[j])))\n",
    "                    edge = edge_dict[(both_side, edge)]\n",
    "                    _i_jedge_dict[i].append((j, edge))\n",
    "            #print(len(edge_dict))\n",
    "            i_jedge_dict = _i_jedge_dict\n",
    "\n",
    "    return np.array(fingerprints)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0Sii9yndxOC"
   },
   "source": [
    "### Step 2: Generate the adjacency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "j-Dg3G7YeGy2"
   },
   "outputs": [],
   "source": [
    "def create_adjacency(mol):\n",
    "    adjacency = Chem.GetAdjacencyMatrix(mol)\n",
    "    adjacency = np.array(adjacency)\n",
    "    adjacency += np.eye(adjacency.shape[0], dtype=int)\n",
    "    return adjacency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TI5qQ_8fJBh"
   },
   "source": [
    "### Step 3: Generate the fingerprints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "S3Z8ANgKfPzG"
   },
   "outputs": [],
   "source": [
    "def get_fingerprints(mol):\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=64, useChirality=True)\n",
    "    return fp.ToBitString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJExOeFofZnd"
   },
   "source": [
    "### Step 4: Generate the protein sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FfCa0vq3dwcj"
   },
   "outputs": [],
   "source": [
    "def split_sequence(sequence, ngram, word_dict):\n",
    "    sequence = '-' + sequence + '='\n",
    "    words = [word_dict[sequence[i:i+ngram]]\n",
    "             for i in range(len(sequence)-ngram+1)]\n",
    "    return np.array(words)\n",
    "\n",
    "# experiment:\n",
    "#split_sequence(primary[1], ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjd5KIXHhNle"
   },
   "source": [
    "### Step 5: Generate the compounds, adjacencies, fps, proteins, interactions with the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "HMdSXDishVeu",
    "outputId": "68a422c3-403e-4988-872d-25a9471b5886"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    output_path = os.path.join('../datasets', task, dataset, name)\\n    os.makedirs(output_path, exist_ok=True)\\n    np.save(os.path.join(output_path, 'compounds'), compounds)\\n    np.save(os.path.join(output_path, 'adjacencies'), adjacencies)\\n    np.save(os.path.join(output_path, 'fingerprint'), fps)\\n    np.save(os.path.join(output_path, 'proteins'), proteins)\\n    np.save(os.path.join(output_path, 'interactions'), interactions)\\n\\n    with open(os.path.join('../datasets', task, dataset, name, 'atom_dict'), 'wb') as f:\\n        pickle.dump(dict(fingerprint_dict), f)\\n    with open(os.path.join('../datasets', task, dataset, name, 'amino_dict'), 'wb') as f:\\n        pickle.dump(dict(word_dict), f)\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_input_data(input_file, dataset, task, name, radius, ngram): \n",
    "# name = train/test, task = interaction/affinity\n",
    "    data = pd.read_csv(input_file, header=None)\n",
    "    data = data[0:1000]\n",
    "    word_dict = defaultdict(lambda: len(word_dict))\n",
    "    fingerprint_dict = defaultdict(lambda: len(fingerprint_dict))\n",
    "    atom_dict = defaultdict(lambda: len(atom_dict))\n",
    "    edge_dict = defaultdict(lambda: len(edge_dict))\n",
    "    bond_dict = defaultdict(lambda: len(bond_dict))\n",
    "    X = []\n",
    "    y = []\n",
    "    compounds, adjacencies, fps, proteins, interactions = [], [], [], [], []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        smiles, sequence, interaction = data.iloc[i, :]\n",
    "\n",
    "        mol = Chem.AddHs(Chem.MolFromSmiles(smiles))\n",
    "        atoms = create_atoms(mol, atom_dict)\n",
    "        i_jbond_dict = create_ijbonddict(mol, bond_dict)\n",
    "        compounds.append(atom_features(atoms, i_jbond_dict, radius, edge_dict, fingerprint_dict))\n",
    "        adjacencies.append(create_adjacency(mol))\n",
    "        fps.append(get_fingerprints(mol))\n",
    "        proteins.append(split_sequence(sequence, ngram, word_dict))\n",
    "        interactions.append(np.array([float(interaction)]))\n",
    "    return [compounds, adjacencies, fps, proteins, interactions], fingerprint_dict, word_dict\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    output_path = os.path.join('../datasets', task, dataset, name)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    np.save(os.path.join(output_path, 'compounds'), compounds)\n",
    "    np.save(os.path.join(output_path, 'adjacencies'), adjacencies)\n",
    "    np.save(os.path.join(output_path, 'fingerprint'), fps)\n",
    "    np.save(os.path.join(output_path, 'proteins'), proteins)\n",
    "    np.save(os.path.join(output_path, 'interactions'), interactions)\n",
    "\n",
    "    with open(os.path.join('../datasets', task, dataset, name, 'atom_dict'), 'wb') as f:\n",
    "        pickle.dump(dict(fingerprint_dict), f)\n",
    "    with open(os.path.join('../datasets', task, dataset, name, 'amino_dict'), 'wb') as f:\n",
    "        pickle.dump(dict(word_dict), f)\n",
    "\"\"\"\n",
    "# experiment with writing the corresponding files:\n",
    "#tokenize_input_data('test_EC50.csv', 'EC50', 'affinity', 'test', 2, 3)\n",
    "#tokenize_input_data('train_EC50.csv', 'EC50', 'affinity', 'train', 2, 3)\n",
    "#tokenize_input_data('test_IC50.csv', 'IC50', 'affinity', 'test', 2, 3)\n",
    "#tokenize_input_data('train_IC50.csv', 'IC50', 'affinity', 'train', 2, 3)\n",
    "#tokenize_input_data('test_Kd.csv', 'Kd', 'affinity', 'test', 2, 3)\n",
    "#tokenize_input_data('train_Kd.csv', 'Kd', 'affinity', 'train', 2, 3)\n",
    "#tokenize_input_data('test_Ki.csv', 'Ki', 'affinity', 'test', 2, 3)\n",
    "#tokenize_input_data('train_Ki.csv', 'Ki', 'affinity', 'train', 2, 3)\n",
    "#tokenize_input_data('test_celegans.csv', 'celegans', 'interaction', 'test', 2, 3)\n",
    "#tokenize_input_data('train_celegans.csv', 'celegans', 'interaction', 'train', 2, 3)\n",
    "#tokenize_input_data('test_human.csv', 'human', 'interaction', 'test', 2, 3)\n",
    "#tokenize_input_data('train_human.csv', 'human', 'interaction', 'train', 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qk5M1O0PiQze"
   },
   "source": [
    "# Part 4: Train the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "SaRqxKk0FStH"
   },
   "outputs": [],
   "source": [
    "affinity_train_data = ['train_EC50.csv', 'train_IC50.csv', 'train_Kd.csv', 'train_Ki.csv']\n",
    "affinity_test_data = ['test_EC50.csv', 'test_IC50.csv', 'test_Kd.csv', 'test_Ki.csv']\n",
    "interaction_train_data = ['train_celegans.csv', 'train_human.csv']\n",
    "interaction_test_data = ['test_celegans.csv', 'test_human.csv']\n",
    "\n",
    "# data = [compounds, adjacencies, fps, proteins, interactions]\n",
    "data, atom_dict, amino_dict = tokenize_input_data(affinity_train_data[0], 'EC50', 'affinity', 'test', 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5LXC0DxHK6aH",
    "outputId": "07952ae1-26f5-4082-8c1c-a2267ccf8569"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 107])\n",
      "torch.Size([1000, 11449])\n",
      "torch.Size([1000, 64])\n",
      "torch.Size([1000, 1434])\n",
      "torch.Size([1000, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rabin badree\\AppData\\Local\\Temp\\ipykernel_2544\\1313135390.py:42: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  label = torch.FloatTensor(batch_data[4]).to(device)\n"
     ]
    }
   ],
   "source": [
    "def batch_pad(arr):\n",
    "    N = max([a.shape[0] for a in arr])\n",
    "    if arr[0].ndim == 1:\n",
    "        new_arr = np.zeros((len(arr), N))\n",
    "        new_arr_mask = np.zeros((len(arr), N))\n",
    "        for i, a in enumerate(arr):\n",
    "            n = a.shape[0]\n",
    "            new_arr[i, :n] = a + 1\n",
    "            new_arr_mask[i, :n] = 1\n",
    "        return new_arr, new_arr_mask\n",
    "\n",
    "    elif arr[0].ndim == 2:\n",
    "        new_arr = np.zeros((len(arr), N, N))\n",
    "        new_arr_mask = np.zeros((len(arr), N, N))\n",
    "        for i, a in enumerate(arr):\n",
    "            n = a.shape[0]\n",
    "            new_arr[i, :n, :n] = a\n",
    "            new_arr_mask[i, :n, :n] = 1\n",
    "        return new_arr, new_arr_mask\n",
    "\n",
    "\n",
    "def fps2number(arr):\n",
    "    new_arr = np.zeros((len(arr), 64))\n",
    "    for i, a in enumerate(arr):\n",
    "        new_arr[i, :] = np.array(list(a), dtype=int)\n",
    "    return new_arr\n",
    "\n",
    "\n",
    "def batch2tensor(batch_data, device):\n",
    "    atoms_pad, atoms_mask = batch_pad(batch_data[0])\n",
    "    adjacencies_pad, _ = batch_pad(batch_data[1])\n",
    "    fps = fps2number(batch_data[2])\n",
    "    amino_pad, amino_mask = batch_pad(batch_data[3])\n",
    "\n",
    "    atoms_pad = Variable(torch.LongTensor(atoms_pad)).to(device)\n",
    "    atoms_mask = Variable(torch.FloatTensor(atoms_mask)).to(device)\n",
    "    adjacencies_pad = Variable(torch.LongTensor(adjacencies_pad)).to(device)\n",
    "    fps = Variable(torch.FloatTensor(fps)).to(device)\n",
    "    amino_pad = Variable(torch.LongTensor(amino_pad)).to(device)\n",
    "    amino_mask = Variable(torch.FloatTensor(amino_mask)).to(device)\n",
    "\n",
    "    label = torch.FloatTensor(batch_data[4]).to(device)\n",
    "\n",
    "    return atoms_pad, atoms_mask, adjacencies_pad, fps, amino_pad, amino_mask, label\n",
    "\n",
    "device = torch.device('cpu')\n",
    "atoms_pad, atoms_mask, adjacencies_pad, batch_fps, amino_pad, amino_mask, label = batch2tensor([data[0],data[1],data[2],data[3],data[4]], device)\n",
    "\n",
    "print(atoms_pad.size())        \n",
    "print(torch.flatten(adjacencies_pad, start_dim=1).size())    \n",
    "print(batch_fps.size())    \n",
    "print(amino_pad.size())   \n",
    "print(label.size())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "zjwjbz8CK6oc"
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(torch.cat((atoms_pad,torch.flatten(adjacencies_pad, start_dim=1),batch_fps, amino_pad), -1).numpy())\n",
    "y = pd.DataFrame(label.numpy())\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DtKotfiheQgW",
    "outputId": "bb97a984-feb5-452e-e5d8-0d1a26125ace"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Columns: 13054 entries, 0 to 13053\n",
      "dtypes: float32(13054)\n",
      "memory usage: 49.8 MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "Qm4SCd2qK61O",
    "outputId": "65b993f9-6f0f-48f8-e3d3-5490bdbffd88"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>13044</th>\n",
       "      <th>13045</th>\n",
       "      <th>13046</th>\n",
       "      <th>13047</th>\n",
       "      <th>13048</th>\n",
       "      <th>13049</th>\n",
       "      <th>13050</th>\n",
       "      <th>13051</th>\n",
       "      <th>13052</th>\n",
       "      <th>13053</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>361.795990</td>\n",
       "      <td>775.789001</td>\n",
       "      <td>702.020996</td>\n",
       "      <td>747.564026</td>\n",
       "      <td>726.793030</td>\n",
       "      <td>784.604980</td>\n",
       "      <td>799.432007</td>\n",
       "      <td>791.070984</td>\n",
       "      <td>741.171997</td>\n",
       "      <td>711.294983</td>\n",
       "      <td>...</td>\n",
       "      <td>24.719999</td>\n",
       "      <td>60.950001</td>\n",
       "      <td>33.540001</td>\n",
       "      <td>62.310001</td>\n",
       "      <td>22.740000</td>\n",
       "      <td>36.549999</td>\n",
       "      <td>23.510000</td>\n",
       "      <td>15.690000</td>\n",
       "      <td>14.400000</td>\n",
       "      <td>62.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>370.343536</td>\n",
       "      <td>585.078308</td>\n",
       "      <td>550.717041</td>\n",
       "      <td>574.165894</td>\n",
       "      <td>599.627686</td>\n",
       "      <td>633.068665</td>\n",
       "      <td>620.204468</td>\n",
       "      <td>619.466553</td>\n",
       "      <td>619.741272</td>\n",
       "      <td>587.457275</td>\n",
       "      <td>...</td>\n",
       "      <td>246.084183</td>\n",
       "      <td>606.748047</td>\n",
       "      <td>333.886230</td>\n",
       "      <td>620.286011</td>\n",
       "      <td>226.373108</td>\n",
       "      <td>363.849884</td>\n",
       "      <td>234.038422</td>\n",
       "      <td>156.191589</td>\n",
       "      <td>143.350082</td>\n",
       "      <td>620.385498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>138.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>247.000000</td>\n",
       "      <td>248.000000</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>242.000000</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>221.000000</td>\n",
       "      <td>591.000000</td>\n",
       "      <td>574.000000</td>\n",
       "      <td>595.500000</td>\n",
       "      <td>588.000000</td>\n",
       "      <td>587.000000</td>\n",
       "      <td>642.000000</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>636.000000</td>\n",
       "      <td>629.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>418.500000</td>\n",
       "      <td>1363.500000</td>\n",
       "      <td>970.250000</td>\n",
       "      <td>1190.000000</td>\n",
       "      <td>1130.000000</td>\n",
       "      <td>1369.000000</td>\n",
       "      <td>1389.000000</td>\n",
       "      <td>1389.000000</td>\n",
       "      <td>1216.000000</td>\n",
       "      <td>1147.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2044.000000</td>\n",
       "      <td>2374.000000</td>\n",
       "      <td>2375.000000</td>\n",
       "      <td>2376.000000</td>\n",
       "      <td>2377.000000</td>\n",
       "      <td>2336.000000</td>\n",
       "      <td>2378.000000</td>\n",
       "      <td>2369.000000</td>\n",
       "      <td>2377.000000</td>\n",
       "      <td>2371.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2472.000000</td>\n",
       "      <td>6095.000000</td>\n",
       "      <td>3354.000000</td>\n",
       "      <td>6231.000000</td>\n",
       "      <td>2274.000000</td>\n",
       "      <td>3655.000000</td>\n",
       "      <td>2351.000000</td>\n",
       "      <td>1569.000000</td>\n",
       "      <td>1440.000000</td>\n",
       "      <td>6232.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 13054 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0            1            2            3            4      \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean    361.795990   775.789001   702.020996   747.564026   726.793030   \n",
       "std     370.343536   585.078308   550.717041   574.165894   599.627686   \n",
       "min      14.000000    14.000000    15.000000    14.000000    14.000000   \n",
       "25%     138.000000   301.000000   247.000000   248.000000   223.000000   \n",
       "50%     221.000000   591.000000   574.000000   595.500000   588.000000   \n",
       "75%     418.500000  1363.500000   970.250000  1190.000000  1130.000000   \n",
       "max    2044.000000  2374.000000  2375.000000  2376.000000  2377.000000   \n",
       "\n",
       "             5            6            7            8            9      ...  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  ...   \n",
       "mean    784.604980   799.432007   791.070984   741.171997   711.294983  ...   \n",
       "std     633.068665   620.204468   619.466553   619.741272   587.457275  ...   \n",
       "min      14.000000    14.000000    14.000000    14.000000    14.000000  ...   \n",
       "25%     223.000000   242.000000   228.000000   207.000000   183.000000  ...   \n",
       "50%     587.000000   642.000000   640.000000   636.000000   629.000000  ...   \n",
       "75%    1369.000000  1389.000000  1389.000000  1216.000000  1147.750000  ...   \n",
       "max    2336.000000  2378.000000  2369.000000  2377.000000  2371.000000  ...   \n",
       "\n",
       "             13044        13045        13046        13047        13048  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean     24.719999    60.950001    33.540001    62.310001    22.740000   \n",
       "std     246.084183   606.748047   333.886230   620.286011   226.373108   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max    2472.000000  6095.000000  3354.000000  6231.000000  2274.000000   \n",
       "\n",
       "             13049        13050        13051        13052        13053  \n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \n",
       "mean     36.549999    23.510000    15.690000    14.400000    62.320000  \n",
       "std     363.849884   234.038422   156.191589   143.350082   620.385498  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "max    3655.000000  2351.000000  1569.000000  1440.000000  6232.000000  \n",
       "\n",
       "[8 rows x 13054 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ThGQewxYpf-",
    "outputId": "935965e7-8048-4d2a-9c93-d114a2d695be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the features data:  (1000, 13054)  No. of Rows: 1000  No. of Columns: 13054\n",
      "Dimension of the target data:  (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimension of the features data: \", X.shape, \" No. of Rows: %d\" % X.shape[0], \" No. of Columns: %d\" % X.shape[1])\n",
    "print(\"Dimension of the target data: \",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pbu5uXo9Ypvw",
    "outputId": "75fd68fd-99a3-4b3f-be59-33fabd0509f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(X.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6P7ZAUafexcm",
    "outputId": "2c8a17ac-b8bc-428a-b523-067dbf44b418"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>5.408936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>5.164310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>7.102373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>4.004365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>7.397940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "687  5.408936\n",
       "500  5.164310\n",
       "332  7.102373\n",
       "979  4.004365\n",
       "817  7.397940"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tWpkPDCVaR3D",
    "outputId": "4b36c047-ffd0-4333-e4f9-c21163ae1017"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:196: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.2981262"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.Ridge(alpha=.5)\n",
    "reg = reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "uAenjwlDFxGg",
    "o8p5oDJDGSFW"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
