{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WRITE WHAT THIS NOTEBOOK IS ABOUT HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAenjwlDFxGg"
   },
   "source": [
    "## Part 1: Prepare the development environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required packages\n",
    "\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import tarfile\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BVNM203br3c8"
   },
   "outputs": [],
   "source": [
    "# functions for processing the data (i.e. extract interaction and affinity data from zipped files)\n",
    "# the output is train/test csv files\n",
    "\n",
    "def extract_data(measure, file):\n",
    "    data = pd.read_csv(file, sep='\\t', dtype=str)\n",
    "    #print(data.columns)\n",
    "    data = data[['Ligand SMILES', 'BindingDB Target Chain  Sequence', 'pKi_[M]', 'pIC50_[M]', 'pKd_[M]', 'pEC50_[M]']]\n",
    "    data.columns = ['SMILES', 'Sequence', 'Ki', 'IC50', 'Kd', 'EC50']\n",
    "    data = data[['SMILES', 'Sequence', measure]]\n",
    "    if 'train' in file:\n",
    "        data.to_csv('../data/csv/train_' + measure + '.csv', index=None, header=None)\n",
    "    else:\n",
    "        data.to_csv('../data/csv/test_' + measure + '.csv', index=None, header=None)\n",
    "\n",
    "def affinity_data_prepare(dataset):\n",
    "    data_dir = '../data/affinity/' + dataset\n",
    "    if not os.path.isdir(data_dir):\n",
    "        tar = tarfile.open(data_dir + '.tar.xz')\n",
    "        os.mkdir(data_dir)\n",
    "        for name in tar.getnames():\n",
    "            tar.extract(name, '../data/affinity/')\n",
    "        tar.close()\n",
    "\n",
    "    extract_data(dataset, data_dir + '/train')\n",
    "    extract_data(dataset, data_dir + '/test')\n",
    "\n",
    "def interaction_data_prepare(dataset):\n",
    "    if dataset == 'human':\n",
    "        train = ('../data/interaction/' + dataset + '/train.txt')\n",
    "        data = pd.read_csv(train, sep=',', dtype=str)\n",
    "        data.columns = ['SMILES', 'Sequence', 'Target']\n",
    "        data.to_csv('../data/csv/train_human.csv', index=None, header=None)\n",
    "        \n",
    "        test = ('../data/interaction/' + dataset + '/test.txt')\n",
    "        data = pd.read_csv(test, sep=',', dtype=str)\n",
    "        data.columns = ['SMILES', 'Sequence', 'Target']\n",
    "        data.to_csv('../data/csv/test_human.csv', index=None, header=None)\n",
    "    else:\n",
    "        train = ('../data/interaction/' + dataset + '/train.txt')\n",
    "        data = pd.read_csv(train, sep=',', dtype=str)\n",
    "        data.columns = ['SMILES', 'Sequence', 'Target']\n",
    "        data.to_csv('../data/csv/train_celegans.csv', index=None, header=None)\n",
    "        \n",
    "        test = ('../data/interaction/' + dataset + '/test.txt')\n",
    "        data = pd.read_csv(test, sep=',', dtype=str)\n",
    "        data.columns = ['SMILES', 'Sequence', 'Target']\n",
    "        data.to_csv('../data/csv/test_celegans.csv', index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for dataset in ['IC50', 'EC50', 'Ki', 'Kd']:\n",
    "    affinity_data_prepare(dataset)\n",
    "    \n",
    "for dataset in ['human', 'celegans']:\n",
    "    interaction_data_prepare(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You should have 12 csv files.\n",
    "\n",
    "<table>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>test_celegans.csv</td>\n",
    "    <td>test_IC50.csv</td>\n",
    "    <td>train_celegans.csv</td>\n",
    "    <td>train_IC50.csv</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>test_EC50.csv</td>\n",
    "    <td>test_Kd.csv</td>\n",
    "    <td>train_EC50.csv</td>\n",
    "    <td>train_Kd.csv</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>test_human.csv</td>\n",
    "    <td>test_Ki.csv</td>\n",
    "    <td>train_human.csv</td>\n",
    "    <td>train_Ki.csv</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiGDHlIYRZsW"
   },
   "source": [
    "## Part 2: Prepare the compound and amino acid representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the directory to that of the CSV files.\n",
    "\n",
    "current = os.getcwd()\n",
    "os.chdir('../data/csv/')\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HO1AjdcUgM2b",
    "outputId": "4b7f7e73-10b0-44b5-fd98-e5d33a595bc4"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('test_EC50.csv', header=None)\n",
    "primary = data.iloc[-1, :]  # three columns : smiles, sequence, interaction \n",
    "radius, ngram = 2, 3\n",
    "mol = Chem.AddHs(Chem.MolFromSmiles(primary[0]))\n",
    "adjacency = Chem.GetAdjacencyMatrix(mol)\n",
    "\n",
    "print([a.GetSymbol() for a in mol.GetAtoms()])\n",
    "print([str(b.GetBondType()) for b in mol.GetBonds()])\n",
    "print([a.GetIdx() for a in mol.GetAromaticAtoms()])\n",
    "\n",
    "#[int(i) for i in AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=64, useChirality=True).ToBitString()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wr0rPhCVGyx3"
   },
   "source": [
    "### 2.1: Generate the data for the Graph Attention Network for the compounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cV3hHD2PSCGK"
   },
   "outputs": [],
   "source": [
    "def create_atoms(mol, atom_dict):\n",
    "# print(primary[0])    \n",
    "# result: C1=CC=C(C(=C1)C2=C(C(=O)NC2=O)NC3=CC(=C(C=C3)O)Cl)[N+](=O)[O-]\n",
    "\n",
    "  atoms = [a.GetSymbol() for a in mol.GetAtoms()]\n",
    "    \n",
    "# print(atoms, len(atoms)) \n",
    "# result:['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'O', 'N', 'C', 'O', 'N', 'C', 'C', 'C', 'C', 'C', 'C', 'O', 'Cl', 'N', 'O', 'O', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H'] 35\n",
    "# print([a.GetIdx() for a in mol.GetAromaticAtoms()])\n",
    "# result:[0, 1, 2, 3, 4, 5, 14, 15, 16, 17, 18, 19]\n",
    "\n",
    "  for a in mol.GetAromaticAtoms():\n",
    "    i = a.GetIdx()\n",
    "    atoms[i] = (atoms[i], 'aromatic')\n",
    "    \n",
    "# print(atoms)\n",
    "# result:[('C', 'aromatic'), ('C', 'aromatic'), ('C', 'aromatic'), ('C', 'aromatic'), ('C', 'aromatic'), ('C', 'aromatic'), 'C', 'C', 'C', 'O', 'N', 'C', 'O', 'N', ('C', 'aromatic'), ('C', 'aromatic'), ('C', 'aromatic'), ('C', 'aromatic'), ('C', 'aromatic'), ('C', 'aromatic'), 'O', 'Cl', 'N', 'O', 'O', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H']\n",
    "\n",
    "  atoms = [atom_dict[a] for a in atoms]\n",
    "    \n",
    "# print(atoms)\n",
    "# result:[0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 3, 1, 2, 3, 0, 0, 0, 0, 0, 0, 2, 4, 3, 2, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
    "\n",
    "  return atoms\n",
    "\n",
    "def create_ijbonddict(mol, bond_dict):\n",
    "    \n",
    "# print(primary[0])    \n",
    "# result: C1=CC=C(C(=C1)C2=C(C(=O)NC2=O)NC3=CC(=C(C=C3)O)Cl)[N+](=O)[O-]\n",
    "\n",
    "    i_jbond_dict = defaultdict(lambda: [])\n",
    "    \n",
    "#print([len(mol.GetBonds())])\n",
    "# result: 37\n",
    "\n",
    "    for b in mol.GetBonds():\n",
    "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        bond = bond_dict[str(b.GetBondType())]\n",
    "        i_jbond_dict[i].append((j, bond))\n",
    "        i_jbond_dict[j].append((i, bond))\n",
    "        \n",
    "#print(i_jbond_dict)\n",
    "# {0: [(1, 0), (5, 0), (25, 1)],\n",
    "#  1: [(0, 0), (2, 0), (26, 1)],\n",
    "#  2: [(1, 0), (3, 0), (27, 1)],\n",
    "#  3: [(2, 0), (4, 0), (22, 1)],\n",
    "#  4: [(3, 0), (5, 0), (6, 1)],\n",
    "#  5: [(4, 0), (0, 0), (28, 1)],\n",
    "#  6: [(4, 1), (7, 2), (11, 1)],\n",
    "#  7: [(6, 2), (8, 1), (13, 1)],\n",
    "#  8: [(7, 1), (9, 2), (10, 1)],\n",
    "#  9: [(8, 2)],\n",
    "#  10: [(8, 1), (11, 1), (29, 1)],\n",
    "#  11: [(10, 1), (12, 2), (6, 1)],\n",
    "#  12: [(11, 2)],\n",
    "#  13: [(7, 1), (14, 1), (30, 1)],\n",
    "#  14: [(13, 1), (15, 0), (19, 0)],\n",
    "#  15: [(14, 0), (16, 0), (31, 1)],\n",
    "#  16: [(15, 0), (17, 0), (21, 1)],\n",
    "#  17: [(16, 0), (18, 0), (20, 1)],\n",
    "#  18: [(17, 0), (19, 0), (32, 1)],\n",
    "#  19: [(18, 0), (14, 0), (33, 1)],\n",
    "#  20: [(17, 1), (34, 1)],\n",
    "#  21: [(16, 1)],\n",
    "#  22: [(3, 1), (23, 2), (24, 1)],\n",
    "#  23: [(22, 2)],\n",
    "#  24: [(22, 1)],\n",
    "#  25: [(0, 1)],\n",
    "#  26: [(1, 1)],\n",
    "#  27: [(2, 1)],\n",
    "#  28: [(5, 1)],\n",
    "#  29: [(10, 1)],\n",
    "#  30: [(13, 1)],\n",
    "#  31: [(15, 1)],\n",
    "#  32: [(18, 1)],\n",
    "#  33: [(19, 1)],\n",
    "#  34: [(20, 1)]})\n",
    "\n",
    "# find the atoms that do not have bonds with other atoms \n",
    "\n",
    "    isolate_atoms = set(range(mol.GetNumAtoms())) - set(i_jbond_dict.keys())\n",
    "    bond = bond_dict['nan']\n",
    "    for a in isolate_atoms:\n",
    "        i_jbond_dict[a].append((a, bond))\n",
    "\n",
    "    return i_jbond_dict\n",
    "\n",
    "def atom_features(atoms, i_jbond_dict, radius, edge_dict, fingerprint_dict):    \n",
    "    if (len(atoms) == 1) or (radius == 0):\n",
    "        fingerprints = [fingerprint_dict[a] for a in atoms]\n",
    "    \n",
    "    else:\n",
    "        nodes = atoms\n",
    "        i_jedge_dict = i_jbond_dict\n",
    "        for _ in range(radius):\n",
    "            fingerprints = []\n",
    "# the first radius:\n",
    "# i_jedge_dict has 35 items. The following loop converts the atom position number(from 0 to 34), into the atoms_type_dict number(totally 5 types).\n",
    "# shrink the previous 35 atoms with edges, into 18 types of (atoms,edges)\n",
    "# fingerprint = (atom_type_dict, tuple(sorted(neighbors)))    \n",
    "# neighbor = (atom_type_dict, bond_type_dict)\n",
    "            for i, j_edge in i_jedge_dict.items():\n",
    "                neighbors = [(nodes[j], edge) for j, edge in j_edge]\n",
    "                fingerprint = (nodes[i], tuple(sorted(neighbors)))\n",
    "                fingerprints.append(fingerprint_dict[fingerprint])  # after converting into atom_type, many fingerprint are with the same value\n",
    "# print([str(b.GetBondType()) for b in mol.GetBonds()])\n",
    "# ['AROMATIC', 'AROMATIC', 'AROMATIC', 'AROMATIC', 'AROMATIC', 'SINGLE', 'DOUBLE', 'SINGLE', 'DOUBLE', 'SINGLE', 'SINGLE', 'DOUBLE', 'SINGLE', 'SINGLE', 'AROMATIC', 'AROMATIC', 'AROMATIC', 'AROMATIC', 'AROMATIC', 'SINGLE', 'SINGLE', 'SINGLE', 'DOUBLE', 'SINGLE', 'AROMATIC', 'SINGLE', 'AROMATIC', 'SINGLE', 'SINGLE', 'SINGLE', 'SINGLE', 'SINGLE', 'SINGLE', 'SINGLE', 'SINGLE', 'SINGLE', 'SINGLE']\n",
    "#print(fingerprints, len(fingerprints))\n",
    "# the first radius of fingerprints: \n",
    "#[0, 0, 0, 1, 2, 0, 3, 4, 5, 6, 7, 5, 6, 8, 1, 0, 9, 10, 0, 0, 11, 12, 13, 14, 15, 16, 16, 16, 16, 17, 17, 16, 16, 16, 18] ---len:35\n",
    "# the second radius of fingerprints: \n",
    "#[0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 11, 12, 13, 14, 15, 16, 1, 17, 18, 19, 20, 21, 22, 22, 22, 22, 23, 24, 22, 22, 22, 25]--with atom type and edge type\n",
    "\n",
    "# update the i_jedge_dict by disgarding the duplicated edges for both directions and only keeping one undirected edge     \n",
    "# upgrade the edge type, from the original edge type, to combine both nodes' type and edge type      \n",
    "            nodes = fingerprints\n",
    "            _i_jedge_dict = defaultdict(lambda: [])\n",
    "            for i, j_edge in i_jedge_dict.items():\n",
    "                for j, edge in j_edge:\n",
    "                    both_side = tuple(sorted((nodes[i], nodes[j])))\n",
    "                    edge = edge_dict[(both_side, edge)]\n",
    "                    _i_jedge_dict[i].append((j, edge))\n",
    "            #print(len(edge_dict))\n",
    "            i_jedge_dict = _i_jedge_dict\n",
    "\n",
    "    return np.array(fingerprints)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0Sii9yndxOC"
   },
   "source": [
    "### 2.2: Generate a second representation of the compounds using extended connectivity fingerprints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3Z8ANgKfPzG"
   },
   "outputs": [],
   "source": [
    "def create_adjacency(mol):\n",
    "    adjacency = Chem.GetAdjacencyMatrix(mol)\n",
    "    adjacency = np.array(adjacency)\n",
    "    adjacency += np.eye(adjacency.shape[0], dtype=int)\n",
    "    return adjacency\n",
    "\n",
    "def get_fingerprints(mol):\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=64, useChirality=True)\n",
    "    return fp.ToBitString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJExOeFofZnd"
   },
   "source": [
    "### 2.3: Generate the data for the Convolutional Neural Network for the proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FfCa0vq3dwcj"
   },
   "outputs": [],
   "source": [
    "def split_sequence(sequence, ngram, word_dict):\n",
    "    sequence = '-' + sequence + '='\n",
    "    words = [word_dict[sequence[i:i+ngram]]\n",
    "             for i in range(len(sequence)-ngram+1)]\n",
    "    return np.array(words)\n",
    "\n",
    "# experiment:\n",
    "#split_sequence(primary[1], ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjd5KIXHhNle"
   },
   "source": [
    "### 2.4: Generate the compounds, adjacencies, fps, proteins, interactions with the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "HMdSXDishVeu",
    "outputId": "68a422c3-403e-4988-872d-25a9471b5886"
   },
   "outputs": [],
   "source": [
    "def tokenize_input_data(input_file, dataset, task, name, radius, ngram): \n",
    "# name = train/test, task = interaction/affinity\n",
    "    data = pd.read_csv(input_file, header=None)\n",
    "    data = data[0:1000]\n",
    "    word_dict = defaultdict(lambda: len(word_dict))\n",
    "    fingerprint_dict = defaultdict(lambda: len(fingerprint_dict))\n",
    "    atom_dict = defaultdict(lambda: len(atom_dict))\n",
    "    edge_dict = defaultdict(lambda: len(edge_dict))\n",
    "    bond_dict = defaultdict(lambda: len(bond_dict))\n",
    "    X = []\n",
    "    y = []\n",
    "    compounds, adjacencies, fps, proteins, interactions = [], [], [], [], []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        smiles, sequence, interaction = data.iloc[i, :]\n",
    "\n",
    "        mol = Chem.AddHs(Chem.MolFromSmiles(smiles))\n",
    "        atoms = create_atoms(mol, atom_dict)\n",
    "        i_jbond_dict = create_ijbonddict(mol, bond_dict)\n",
    "        compounds.append(atom_features(atoms, i_jbond_dict, radius, edge_dict, fingerprint_dict))\n",
    "        adjacencies.append(create_adjacency(mol))\n",
    "        fps.append(get_fingerprints(mol))\n",
    "        proteins.append(split_sequence(sequence, ngram, word_dict))\n",
    "        interactions.append(np.array([float(interaction)]))\n",
    "    return [compounds, adjacencies, fps, proteins, interactions], fingerprint_dict, word_dict\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    output_path = os.path.join('../datasets', task, dataset, name)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    np.save(os.path.join(output_path, 'compounds'), compounds)\n",
    "    np.save(os.path.join(output_path, 'adjacencies'), adjacencies)\n",
    "    np.save(os.path.join(output_path, 'fingerprint'), fps)\n",
    "    np.save(os.path.join(output_path, 'proteins'), proteins)\n",
    "    np.save(os.path.join(output_path, 'interactions'), interactions)\n",
    "\n",
    "    with open(os.path.join('../datasets', task, dataset, name, 'atom_dict'), 'wb') as f:\n",
    "        pickle.dump(dict(fingerprint_dict), f)\n",
    "    with open(os.path.join('../datasets', task, dataset, name, 'amino_dict'), 'wb') as f:\n",
    "        pickle.dump(dict(word_dict), f)\n",
    "\"\"\"\n",
    "# experiment with writing the corresponding files:\n",
    "#tokenize_input_data('test_EC50.csv', 'EC50', 'affinity', 'test', 2, 3)\n",
    "#tokenize_input_data('train_EC50.csv', 'EC50', 'affinity', 'train', 2, 3)\n",
    "#tokenize_input_data('test_IC50.csv', 'IC50', 'affinity', 'test', 2, 3)\n",
    "#tokenize_input_data('train_IC50.csv', 'IC50', 'affinity', 'train', 2, 3)\n",
    "#tokenize_input_data('test_Kd.csv', 'Kd', 'affinity', 'test', 2, 3)\n",
    "#tokenize_input_data('train_Kd.csv', 'Kd', 'affinity', 'train', 2, 3)\n",
    "#tokenize_input_data('test_Ki.csv', 'Ki', 'affinity', 'test', 2, 3)\n",
    "#tokenize_input_data('train_Ki.csv', 'Ki', 'affinity', 'train', 2, 3)\n",
    "#tokenize_input_data('test_celegans.csv', 'celegans', 'interaction', 'test', 2, 3)\n",
    "#tokenize_input_data('train_celegans.csv', 'celegans', 'interaction', 'train', 2, 3)\n",
    "#tokenize_input_data('test_human.csv', 'human', 'interaction', 'test', 2, 3)\n",
    "#tokenize_input_data('train_human.csv', 'human', 'interaction', 'train', 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qk5M1O0PiQze"
   },
   "source": [
    "## Part 3: Train the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SaRqxKk0FStH"
   },
   "outputs": [],
   "source": [
    "affinity_train_data = ['train_EC50.csv', 'train_IC50.csv', 'train_Kd.csv', 'train_Ki.csv']\n",
    "affinity_test_data = ['test_EC50.csv', 'test_IC50.csv', 'test_Kd.csv', 'test_Ki.csv']\n",
    "interaction_train_data = ['train_celegans.csv', 'train_human.csv']\n",
    "interaction_test_data = ['test_celegans.csv', 'test_human.csv']\n",
    "\n",
    "# data = [compounds, adjacencies, fps, proteins, interactions]\n",
    "data, atom_dict, amino_dict = tokenize_input_data(affinity_train_data[0], 'EC50', 'affinity', 'test', 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5LXC0DxHK6aH",
    "outputId": "07952ae1-26f5-4082-8c1c-a2267ccf8569"
   },
   "outputs": [],
   "source": [
    "def batch_pad(arr):\n",
    "    N = max([a.shape[0] for a in arr])\n",
    "    if arr[0].ndim == 1:\n",
    "        new_arr = np.zeros((len(arr), N))\n",
    "        new_arr_mask = np.zeros((len(arr), N))\n",
    "        for i, a in enumerate(arr):\n",
    "            n = a.shape[0]\n",
    "            new_arr[i, :n] = a + 1\n",
    "            new_arr_mask[i, :n] = 1\n",
    "        return new_arr, new_arr_mask\n",
    "\n",
    "    elif arr[0].ndim == 2:\n",
    "        new_arr = np.zeros((len(arr), N, N))\n",
    "        new_arr_mask = np.zeros((len(arr), N, N))\n",
    "        for i, a in enumerate(arr):\n",
    "            n = a.shape[0]\n",
    "            new_arr[i, :n, :n] = a\n",
    "            new_arr_mask[i, :n, :n] = 1\n",
    "        return new_arr, new_arr_mask\n",
    "\n",
    "\n",
    "def fps2number(arr):\n",
    "    new_arr = np.zeros((len(arr), 64))\n",
    "    for i, a in enumerate(arr):\n",
    "        new_arr[i, :] = np.array(list(a), dtype=int)\n",
    "    return new_arr\n",
    "\n",
    "\n",
    "def batch2tensor(batch_data, device):\n",
    "    atoms_pad, atoms_mask = batch_pad(batch_data[0])\n",
    "    adjacencies_pad, _ = batch_pad(batch_data[1])\n",
    "    fps = fps2number(batch_data[2])\n",
    "    amino_pad, amino_mask = batch_pad(batch_data[3])\n",
    "\n",
    "    atoms_pad = Variable(torch.LongTensor(atoms_pad)).to(device)\n",
    "    atoms_mask = Variable(torch.FloatTensor(atoms_mask)).to(device)\n",
    "    adjacencies_pad = Variable(torch.LongTensor(adjacencies_pad)).to(device)\n",
    "    fps = Variable(torch.FloatTensor(fps)).to(device)\n",
    "    amino_pad = Variable(torch.LongTensor(amino_pad)).to(device)\n",
    "    amino_mask = Variable(torch.FloatTensor(amino_mask)).to(device)\n",
    "\n",
    "    label = torch.FloatTensor(batch_data[4]).to(device)\n",
    "\n",
    "    return atoms_pad, atoms_mask, adjacencies_pad, fps, amino_pad, amino_mask, label\n",
    "\n",
    "device = torch.device('cpu')\n",
    "atoms_pad, atoms_mask, adjacencies_pad, batch_fps, amino_pad, amino_mask, label = batch2tensor([data[0],data[1],data[2],data[3],data[4]], device)\n",
    "\n",
    "print(atoms_pad.size())        \n",
    "print(torch.flatten(adjacencies_pad, start_dim=1).size())    \n",
    "print(batch_fps.size())    \n",
    "print(amino_pad.size())   \n",
    "print(label.size())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjwjbz8CK6oc"
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(torch.cat((atoms_pad,torch.flatten(adjacencies_pad, start_dim=1),batch_fps, amino_pad), -1).numpy())\n",
    "y = pd.DataFrame(label.numpy())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DtKotfiheQgW",
    "outputId": "bb97a984-feb5-452e-e5d8-0d1a26125ace"
   },
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "Qm4SCd2qK61O",
    "outputId": "65b993f9-6f0f-48f8-e3d3-5490bdbffd88"
   },
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ThGQewxYpf-",
    "outputId": "935965e7-8048-4d2a-9c93-d114a2d695be"
   },
   "outputs": [],
   "source": [
    "print(\"Dimension of the features data: \", X.shape, \" No. of Rows: %d\" % X.shape[0], \" No. of Columns: %d\" % X.shape[1])\n",
    "print(\"Dimension of the target data: \",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pbu5uXo9Ypvw",
    "outputId": "75fd68fd-99a3-4b3f-be59-33fabd0509f6"
   },
   "outputs": [],
   "source": [
    "sum(X.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6P7ZAUafexcm",
    "outputId": "2c8a17ac-b8bc-428a-b523-067dbf44b418"
   },
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Create and test a Ridge Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tWpkPDCVaR3D",
    "outputId": "4b36c047-ffd0-4333-e4f9-c21163ae1017"
   },
   "outputs": [],
   "source": [
    "reg = linear_model.Ridge(alpha=.5)\n",
    "reg = reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "uAenjwlDFxGg",
    "o8p5oDJDGSFW"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
