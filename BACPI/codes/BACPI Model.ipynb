{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cv-bVr-hGaM6"
   },
   "source": [
    "# Part 1: Prepare the development environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C9EaJ7QvG3NG"
   },
   "outputs": [],
   "source": [
    "# import the required packages\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from transformers import AutoTokenizer, EsmForSequenceClassification\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following data files are used in this notebook: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Tokenize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5R9MXacHOIz"
   },
   "outputs": [],
   "source": [
    "def token(data,model_name):\n",
    "  y=data.iloc[:, -1]\n",
    "  print(\"labels:\", y.shape)\n",
    "  X = []\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "  for i in range(len(data)):\n",
    "    compound, protein, interaction = data.iloc[i, :]\n",
    "    dict_item = {}\n",
    "    mol = Chem.AddHs(Chem.MolFromSmiles(compound))\n",
    "    fp = np.array([int(i) for i in AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=4, useChirality=True).ToBitString()])\n",
    "    pn = tokenizer(protein)\n",
    "    input_item= torch.from_numpy(np.array([np.concatenate((pn['input_ids'],fp), axis=0)], dtype='int32'))\n",
    "    mask_item = torch.from_numpy(np.array([np.concatenate((pn['attention_mask'],np.ones(len(fp))), axis=0)], dtype='int32'))\n",
    "    dict_item['input_ids']= input_item\n",
    "    dict_item['attention_mask']= mask_item\n",
    "    #print(dict_item)\n",
    "    X.append(dict_item)\n",
    "\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yf6PVgujGXBx"
   },
   "source": [
    "### Future Work: What is that future work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bst2VBOrGUjQ"
   },
   "outputs": [],
   "source": [
    "# the aim to assign the fingerprint of compound as larger tokenizor, for example making fingerprints set {0,1} as set {40,50}, is to make compound data distinct from protein data\n",
    "# currently the maximum index in ESM tokenization cannot support this idea, so this project keeps the fingerprints as set {0,1}\n",
    "\n",
    "fp_original = np.array([int(i) for i in AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=4, useChirality=True).ToBitString()])\n",
    "fp_update = np.where(fp_original==1, 40, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTUgmvpFH7K1"
   },
   "outputs": [],
   "source": [
    "# (1) the order of compound & protein in the combined sequence may impact the final result\n",
    "# (2) the position of compound inserted in the protein sequence may impact the final result\n",
    "\n",
    "#input_item= torch.from_numpy(np.array([np.concatenate((fp,pn['input_ids']), axis=0)], dtype='int32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzUFA1YkRVeZ"
   },
   "source": [
    "# Part 3: Pretrain Protein Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tnd6H6QWei_"
   },
   "outputs": [],
   "source": [
    "def feature_extraction(input):\n",
    "\n",
    "  with torch.no_grad():\n",
    "    item = model(**input, labels = torch.tensor([y.iloc[i]]))\n",
    "    logits = item.logits\n",
    "    loss = item.loss\n",
    "    #print(\"The original feature extraction is :\",\"features as\",logits,\"loss compared with labels as\", loss)\n",
    "\n",
    "  return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7CR5xU58SFPV",
    "outputId": "58bbfd98-19bf-4743-9403-ed053e331de3"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('test_celegans.csv', header=None)\n",
    "data = data.sample(frac=1)\n",
    "model_name = \"facebook/esm2_t6_8M_UR50D\"\n",
    "inputs,y = token(data,model_name)\n",
    "\n",
    "model = EsmForSequenceClassification.from_pretrained(model_name, num_labels=len(y))\n",
    "features = []\n",
    "for i in range(len(inputs)):\n",
    "  input = inputs[i]\n",
    "  logits, loss = feature_extraction(input)\n",
    "  feature = logits.numpy()[0]\n",
    "  features.append(feature)\n",
    "\n",
    "features = pd.DataFrame(features)\n",
    "log = MLPClassifier(random_state=42, hidden_layer_sizes=(5, ), alpha=0.01, max_iter=200,  activation='logistic')\n",
    "log.fit(features,y)\n",
    "y_pred = log.predict(features)\n",
    "print(\"RMSE:\", np.sqrt(np.mean((y_pred - y) ** 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0197qsyMsf-"
   },
   "source": [
    "### Future Work: LANGUAGE MODELING WITH NN.TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3Fx0S4HLUL4"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of ``-inf``, with zeros on ``diag``.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_d7LQ_zfLdaY"
   },
   "outputs": [],
   "source": [
    "# PositionalEncoding module injects some information about the relative or absolute position of the tokens in the sequence. \n",
    "# The positional encodings have the same dimension as the embeddings so that the two can be summed. \n",
    "# Here, we use sine and cosine functions of different frequencies.\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# reference : https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNy_t_FBcsG5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
